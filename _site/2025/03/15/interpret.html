<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A bit about Interpretability</title>
    <meta name="description" content="Phong BH. Nguyen's personal page - Scientist, programmer, adventurer. Learn about my research, projects, and interests.">
    <meta name="keywords" content="Phong BH. Nguyen, bioinformatics, data science, machine learning, health informatics, non-communicable diseases, diabetes, cancer, cardiovascular diseases">
    <meta name="author" content="Phong BH. Nguyen">
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>A bit about Interpretability</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="A bit about Interpretability" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Interpretability in machine learning is one of the topics I paid attention to throughout my PhD. Especially in biomedical research, building models that predict outcomes accurately is thrilling, but it’s not the entire story. The magic happens when we truly understand what these models tell us and translate those insights into something meaningful. In this post, let’s dive into why interpretability matters and explore some practical ways to make our models both understandable and impactful." />
<meta property="og:description" content="Interpretability in machine learning is one of the topics I paid attention to throughout my PhD. Especially in biomedical research, building models that predict outcomes accurately is thrilling, but it’s not the entire story. The magic happens when we truly understand what these models tell us and translate those insights into something meaningful. In this post, let’s dive into why interpretability matters and explore some practical ways to make our models both understandable and impactful." />
<link rel="canonical" href="http://localhost:4000/2025/03/15/interpret.html" />
<meta property="og:url" content="http://localhost:4000/2025/03/15/interpret.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-15T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A bit about Interpretability" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-15T00:00:00+01:00","datePublished":"2025-03-15T00:00:00+01:00","description":"Interpretability in machine learning is one of the topics I paid attention to throughout my PhD. Especially in biomedical research, building models that predict outcomes accurately is thrilling, but it’s not the entire story. The magic happens when we truly understand what these models tell us and translate those insights into something meaningful. In this post, let’s dive into why interpretability matters and explore some practical ways to make our models both understandable and impactful.","headline":"A bit about Interpretability","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/03/15/interpret.html"},"url":"http://localhost:4000/2025/03/15/interpret.html"}</script>
<!-- End Jekyll SEO tag -->

    <style>
      body {
        background: rgba(64,64,64,0.8);
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        flex-direction: column;
        height: 100vh;
        color: white;
        font-size: 2vw;
      }
      .tabs {
        display: flex;
        justify-content: center;
        align-items: center;
        width: 100%;
        padding: 0.8vw;
        background: rgba(43, 43, 43, 1); /* Coffee color with reduced opacity */
        font-size: 3vw;
        position: fixed;
      }
      .tabs a {
        color: #F7CE46;
        text-decoration: none;
        margin: 0 10px;
        padding: 10px;
        background: none;
        border-radius: 1vw;
        font-size: 1.3vw;
      }
      .tabs a.current {
        background: #F7CE46; /* Different color for the current page */
        color: rgba(43, 43, 43, 1);
        border-radius: 1vw;
        font-size: 1.5vw;
      }
      .menu-button {
        display: none; /* Hide by default */
      }
      .toc-button {
        display: none; /* Hide by default */
      }
      .container {
        display: flex;
        flex: 1;
      }
      .left-bar {
        background: rgba(51, 51, 51, 0.9); /* Coffee color with reduced opacity */
        width: 16vw;
        padding: 0.5vw;
        position: fixed;
        top: 5vw;
        display: flex;
        flex-direction: column;
        align-items: center;
        font-size: 2.5vw;
      }
      .left-bar img {
        border-radius: 50%;
        width: 10vw;
        height: 10vw;
        object-fit: cover;
        margin-bottom: 0.5vw;
        margin-left: 0.8vw;
      }
      .left-bar ul {
        list-style: none;
        padding: 0;
      }
      .left-bar ul li {
        margin: 0.3vw 0.3vw;
      }
      .left-bar ul li a {
        color: rgb(221,221,221);
        text-decoration: none;
        font-size: 1.2vw;
      }
      .content {
        flex: 1;
        padding: 40px;
        background: rgba(64,64,64,0.8); /* Different color for the content area */
        color: rgb(221,221,221);
        font-size: 1vw;
        margin-left: 230px;
        margin-top: 40px;
        text-align: justify;
      }
      .content a {
        color: #F7CE46; /* Change hyperlink color */
      }
      /* Custom styles for headers */
      h1, h2, h3, h4, h5, h6 {
        color: #F7CE46; /* Change header color */
        background-color: rgba(43, 43, 43, 0.8); /* Highlight background color */
        padding: 5px; /* Add padding */
        border-radius: 5px; /* Add border radius */
      }
      .project-item {
        clear: both;
        margin-bottom: 30px;
      }
      @media (max-aspect-ratio: 1/1) {
        body {
          font-size: 2vw; /* Adjust font size for smaller screens */
        }
        .tabs {
          justify-content: space-between;
          padding: 0px;
          font-size: 8px; /* Adjust font size for smaller screens */
        }
        .tabs a {
          display: none; /* Hide tabs on phone screens */
        }
        .tabs a.current {
          font-size: 4vw;
        }
        .menu-button {
          display: block; /* Show menu button on phone screens */
          background: #F7CE46;
          color: rgba(43, 43, 43, 1);
          border: none;
          padding: 10px;
          font-size: 4vw;
          cursor: pointer;
          position: fixed;
          border-radius: 5px;
          top: 10px;
          right: 20px;
        }
        .menu-content {
          display: none;
          position: absolute;
          top: 60px;
          right: 20px;
          background: rgba(43, 43, 43, 1);
          padding: 10px;
          border-radius: 5px;
          z-index: 1001;
        }
        .menu-content a {
          display: block;
          color: #F7CE46;
          text-decoration: none;
          margin: 5px 0;
          padding: 5px;
          font-size: 4vw;
        }
        .left-bar {
          width: 16%; /* Adjust width for smaller screens */
          padding: 4px;
          font-size: 3vw; /* Adjust font size for smaller screens */
          top: 7px;
          border-radius: 3px;
        }
        .left-bar img {
          width: 8vw; /* Smaller image for smaller screens */
          height: 8vw;
          margin-bottom: 2px;
          margin-left: 5px;
        }
        .left-bar ul li {
          margin: 5px 0px;
        }
        .left-bar ul li a {
          font-size: 3vw; /* Adjust font size for smaller screens */
          padding: 2px;
          margin-top: 2px;
        }
        .toc-button {
          display: block; /* Show table of content button on phone screens */
          background: #F7CE46;
          color: rgba(43, 43, 43, 1);
          border: none;
          padding: 3px 3px;
          font-size: 2vw;
          cursor: pointer;
          border-radius: 3px;
          margin-top: 3px;
        }
        .toc-content {
          display: none;
          background: rgba(51, 51, 51, 0.9);
          color: #F7CE46;
          padding: 8px;
          border-radius: 5px;
          margin-left: 3px;
          width: 100%;
          font-size: 3vw;
        }
        .toc-content a {
          display: block;
          text-decoration: none;
          margin: 5px 0;
          padding: 5px;
          font-size: 3vw;
        }
        .content {
          padding: 15px;
          font-size: 3vw; /* Adjust font size for smaller screens */
          margin-top: 0px;
          margin-left: 15vw;
          text-align: justify;
        }
        .project-item {
          margin-left: 0px;
          list-style-type: none; /* Remove default list styling */
        }
        ul {
          padding: 0; /* Remove default padding */
        }
      }
    </style>
  </head>
  <body>
    <div class="tabs">
      <button class="menu-button" onclick="toggleMenu()">Menu</button>
      <div class="menu-content" id="menu-content">
        <nav>
  
    <a href="/">Home</a>
  
    <a href="/about.html">About me</a>
  
    <a href="/projects.html">Projects/ tools development</a>
  
    <a href="/data_viz.html">Data visualization</a>
  
    <a href="/blog.html">Blog</a>
  
</nav>
      </div>
    </div>
    <div class="container">
      <div class="left-bar">
        <img src="/assets/images/page_img.jpg" alt="Profile Picture">
        <button class="toc-button" onclick="toggleToc()">Table of Content</button>
        <div class="toc-content" id="toc-content">
          <ul>
            
              <li><a href="#chapter 1">Introduction</a></li>
            
              <li><a href="#chapter2">Interpretability in biomedical research</a></li>
            
              <li><a href="#chapter3">Inherently interpretable algorithms</a></li>
            
              <li><a href="#chapter4">Posthoc interpretation for deep learning models</a></li>
            
              <li><a href="#chapter5">Incorporating biological knowledge</a></li>
            
              <li><a href="#chapter6">Final remarks</a></li>
            
          </ul>
        </div>
      </div>
      <div class="content">
        <h1>A bit about Interpretability</h1>

<p>
  15 Mar 2025
  
  
</p>

<p><a id="chapter1"></a>
Interpretability in machine learning is one of the topics I paid attention to throughout my PhD. Especially in biomedical research, building models that predict outcomes accurately is thrilling, but it’s not the entire story. The magic happens when we truly understand what these models tell us and translate those insights into something meaningful. In this post, let’s dive into why interpretability matters and explore some practical ways to make our models both understandable and impactful.</p>

<h3 id="1-interpretability-in-biomedical-research">1. Interpretability in Biomedical research</h3>
<p><a id="chapter2"></a>
Interpretability isn’t just a fancy word—it’s crucial for translating ML models into real-world biomedical applications. It helps scientists uncover biomarkers, provides deeper mechanistic insights into diseases, and assists biotech and pharma in drug discovery. Equally important, interpretability boosts trust: doctors, researchers, and even patients can better accept and use ML tools when they understand what’s behind predictions.</p>

<p>Simpler models often provide better interpretability. For instance, let’s take a look at this cute picture comparing a decision tree and a deep neural network:</p>

<div style="text-align: center;">
  <img src="/assets/images/interpretability.jpeg" alt="Figure 1" style="max-width: 70%; height: auto;" />
  <p style="max-width: 80%; margin: auto; font-size: 10px;"><strong>Comparison between decision tree and deep neural network.</strong></p> 
</div>

<p>In the decision tree, we see exactly how decisions are made (e.g., age and gender directly influence the risk, and age is relatively more important). It’s as clear as preparing an organic fruit salad—you see exactly what’s going in, fresh and straightforward! Conversely, the deep neural network is more like a premade smoothie—delicious but with mysterious ingredients blended together, leaving you guessing what’s inside!</p>

<h3 id="2-inherently-interpretable-algorithms">2. Inherently interpretable algorithms</h3>
<p><a id="chapter3"></a>
In biomedical research, we often prefer simpler models such as linear regression, logistic regression, or decision trees. Why? Because most biomedical studies typically have smaller sample sizes and limited computational resources. Besides, simpler models generally perform reasonably well for tasks like classification and forecasting.</p>

<p>These simpler models come with inherent interpretability, meaning you can directly extract feature importance from the model parameters themselves. For example:</p>

<ul>
  <li>
    <p><strong>Linear models</strong>: The coefficients directly indicate the importance and direction (positive or negative influence) of each feature. Like picking out individual fruits, you clearly see what and how much contributes to your tasty salad.</p>
  </li>
  <li>
    <p><strong>Tree-based models</strong> (Decision Trees, Random Forests): You can explicitly visualize and interpret how each feature influences predictions through tree splits. Feature importance can be quantified by how much each feature reduces uncertainty or error—each decision step.</p>
  </li>
</ul>

<p>Thus, choosing simpler algorithms often gives you immediate interpretability, making your life easier—no detective work needed here, just fresh ingredients!</p>

<h3 id="3-posthoc-interpretability-in-deep-learning">3. Posthoc interpretability in deep learning</h3>
<p><a id="chapter4"></a>
But what if we have sufficient samples, resources, and a strong desire to push predictive power to the limits? Deep learning is our superhero, though it usually comes wrapped in a complex, mysterious package—much like a premade smoothie. Tasty? Yes. Transparent? Not always!</p>

<p>Fortunately, we have posthoc methods to make sense of these complex models after they’ve been trained:</p>

<ul>
  <li>
    <p><a href="https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">SHAP (SHapley Additive exPlanations)</a> : SHAP assigns each feature an importance value for a particular prediction, inspired by cooperative game theory. It explains individual predictions clearly by measuring each feature’s contribution.</p>
  </li>
  <li>
    <p><a href="https://proceedings.mlr.press/v70/sundararajan17a.html">Integrated Gradients</a> : Integrated gradients measure the contribution of each input feature by integrating gradients along the path from a baseline input (often zero) to the actual input. It reveals how changing each input slightly affects the model’s prediction.</p>
  </li>
</ul>

<p>These methods act like nutritional labels or ingredient lists—turning your mysterious smoothie into something you can confidently savor!</p>

<h3 id="4-incorporation-of-biological-knowledge">4. Incorporation of biological knowledge</h3>
<p><a id="chapter5"></a>
Adding biological expertise is like sprinkling some expert seasoning that you know exactly what it does onto your dish—it elevates the entire analysis. By integrating prior biological knowledge during feature selection, we do not only narrow down the search space to “relevant features”, but also enhance interpretability by associating our features with prior biological knowledge that might boost mechanistic explaination. Resources like databases for molecular attributes and phenotype associations (e.g., KEGG, Reactome, Gene Ontology) significantly empower our interpretations.</p>

<p>However, be cautious here, biological knowledge is limited. The current available knowledge might not be sufficient to explain all the variance within our examined dataset. Always pair it with unbiased approaches to ensure comprehensive and balanced analyses.</p>

<h3 id="final-remarks">Final remarks</h3>
<p><a id="chapter6"></a>
Interpretability isn’t just icing on the ML cake—it’s the flour holding it all together. By prioritizing understandable models and cleverly combining computational methods with biological knowledge, we can unlock the true potential of ML in biomedical research.</p>

<p>Above are some thoughts for building ML models we can trust, understand, and genuinely enjoy unraveling!</p>

<p>Happy interpreting!</p>

      </div>
    </div>
    <script>
      function toggleMenu() {
        var menuContent = document.getElementById('menu-content');
        if (menuContent.style.display === 'block') {
          menuContent.style.display = 'none';
        } else {
          menuContent.style.display = 'block';
        }
      }
      // Hide menu when clicking outside of it
      document.addEventListener('click', function(event) {
        var menu = document.getElementById('menu-content');
        var menuButton = document.querySelector('.menu-button');
        if (menu.style.display === 'block' && !menu.contains(event.target) && !menuButton.contains(event.target)) {
            menu.style.display = 'none';
        }
      });
      function toggleToc() {
        var tocContent = document.getElementById('toc-content');
        if (tocContent.style.display === 'block') {
          tocContent.style.display = 'none';
        } else {
          tocContent.style.display = 'block';
        }
      }
      // Hide menu when clicking outside of it
      document.addEventListener('click', function(event) {
        var menu = document.getElementById('toc-content');
        var menuButton = document.querySelector('.toc-button');
        if (menu.style.display === 'block' && !menu.contains(event.target) && !menuButton.contains(event.target)) {
            menu.style.display = 'none';
        }
      });
    </script>
  </body>
</html>